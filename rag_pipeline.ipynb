{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval-Augmented Generation (RAG) Pipeline\n",
    "## AI Engineering Assignment\n",
    "\n",
    "This notebook demonstrates a complete RAG pipeline using LangChain:\n",
    "1. Load sample documents\n",
    "2. Split into chunks\n",
    "3. Create embeddings\n",
    "4. Store in FAISS vector database\n",
    "5. Retrieve relevant chunks\n",
    "6. Generate answer using LLM\n",
    "\n",
    "**Important Setup:**\n",
    "- Make sure you're using the **\"RAG Assignment (Python 3.9)\"** kernel\n",
    "- All packages are pre-installed in the virtual environment\n",
    "- You'll need an OpenAI API key or Anthropic API key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Verify Libraries\n",
    "\n",
    "All packages are pre-installed in the dedicated virtual environment. Let's verify they're available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All libraries loaded successfully!\n",
      "✓ LangChain version: 0.3.27\n",
      "✓ FAISS available\n",
      "\n",
      "Ready to build RAG pipeline!\n"
     ]
    }
   ],
   "source": [
    "# Verify installations (packages are pre-installed in virtual environment)\n",
    "import langchain\n",
    "import faiss\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "print(\"✓ All libraries loaded successfully!\")\n",
    "print(f\"✓ LangChain version: {langchain.__version__}\")\n",
    "print(f\"✓ FAISS available\")\n",
    "print(f\"\\nReady to build RAG pipeline!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Configure API Keys\n",
    "\n",
    "You'll need an API key for the chat model (Claude or OpenAI).\n",
    "\n",
    "**Recommendation**: Use **Anthropic Claude** - more generous free tier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Anthropic API key configured\n",
      "  Using: Claude-3.5-Haiku for chat (fast & affordable)\n",
      "\n",
      "Note: Embeddings use FREE HuggingFace (no API credits needed!)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "# ============================================\n",
    "# Choose which LLM API to use\n",
    "# ============================================\n",
    "USE_OPENAI = False  # Set to True to use OpenAI instead of Claude\n",
    "\n",
    "# For embeddings, we use FREE HuggingFace (no API key needed)\n",
    "# For chat LLM, you need either Anthropic or OpenAI API key\n",
    "\n",
    "if USE_OPENAI:\n",
    "    if \"OPENAI_API_KEY\" not in os.environ:\n",
    "        os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")\n",
    "    print(\"✓ OpenAI API key configured\")\n",
    "    print(\"  Using: GPT-3.5-turbo for chat\")\n",
    "else:\n",
    "    if \"ANTHROPIC_API_KEY\" not in os.environ:\n",
    "        os.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass(\"Enter your Anthropic API key: \")\n",
    "    print(\"✓ Anthropic API key configured\")\n",
    "    print(\"  Using: Claude-3.5-Haiku for chat (fast & affordable)\")\n",
    "\n",
    "print(\"\\nNote: Embeddings use FREE HuggingFace (no API credits needed!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load Sample Documents\n",
    "Creating sample text documents about AI topics for our knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 6 documents\n",
      "\n",
      "Document sources:\n",
      "1. ai_basics.txt - AI Introduction\n",
      "   Content preview: Artificial Intelligence (AI) is the simulation of human intelligence processes by machines, \n",
      "       ...\n",
      "\n",
      "2. machine_learning.txt - Machine Learning\n",
      "   Content preview: Machine Learning is a subset of artificial intelligence that focuses on the development of \n",
      "        ...\n",
      "\n",
      "3. deep_learning.txt - Deep Learning\n",
      "   Content preview: Deep Learning is a specialized subset of machine learning that uses neural networks with \n",
      "        mu...\n",
      "\n",
      "4. nlp.txt - NLP\n",
      "   Content preview: Natural Language Processing (NLP) is a branch of artificial intelligence that helps computers \n",
      "     ...\n",
      "\n",
      "5. rag.txt - RAG\n",
      "   Content preview: Retrieval-Augmented Generation (RAG) is a technique that combines information retrieval with \n",
      "      ...\n",
      "\n",
      "6. vector_db.txt - Vector Databases\n",
      "   Content preview: Vector databases are specialized databases designed to store and efficiently search through \n",
      "       ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "# Create sample documents about AI and Machine Learning\n",
    "documents = [\n",
    "    Document(\n",
    "        page_content=\"\"\"Artificial Intelligence (AI) is the simulation of human intelligence processes by machines, \n",
    "        especially computer systems. These processes include learning, reasoning, and self-correction. \n",
    "        AI applications include expert systems, natural language processing, speech recognition, and machine vision. \n",
    "        AI has become increasingly important in modern technology and is used in various industries including \n",
    "        healthcare, finance, transportation, and entertainment.\"\"\",\n",
    "        metadata={\"source\": \"ai_basics.txt\", \"topic\": \"AI Introduction\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"\"\"Machine Learning is a subset of artificial intelligence that focuses on the development of \n",
    "        algorithms and statistical models that enable computers to learn and improve from experience without being \n",
    "        explicitly programmed. There are three main types of machine learning: supervised learning, unsupervised learning, \n",
    "        and reinforcement learning. Supervised learning uses labeled data, unsupervised learning finds patterns in \n",
    "        unlabeled data, and reinforcement learning learns through trial and error with rewards.\"\"\",\n",
    "        metadata={\"source\": \"machine_learning.txt\", \"topic\": \"Machine Learning\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"\"\"Deep Learning is a specialized subset of machine learning that uses neural networks with \n",
    "        multiple layers (deep neural networks). These networks are inspired by the structure and function of the human brain. \n",
    "        Deep learning has achieved remarkable success in areas such as image recognition, natural language processing, \n",
    "        and game playing. Popular deep learning frameworks include TensorFlow, PyTorch, and Keras. Deep learning models \n",
    "        require large amounts of data and computational power to train effectively.\"\"\",\n",
    "        metadata={\"source\": \"deep_learning.txt\", \"topic\": \"Deep Learning\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"\"\"Natural Language Processing (NLP) is a branch of artificial intelligence that helps computers \n",
    "        understand, interpret, and manipulate human language. NLP combines computational linguistics with statistical, \n",
    "        machine learning, and deep learning models. Applications of NLP include machine translation, sentiment analysis, \n",
    "        chatbots, text summarization, and question answering systems. Modern NLP has been revolutionized by transformer \n",
    "        models like BERT and GPT.\"\"\",\n",
    "        metadata={\"source\": \"nlp.txt\", \"topic\": \"NLP\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"\"\"Retrieval-Augmented Generation (RAG) is a technique that combines information retrieval with \n",
    "        text generation. RAG systems first retrieve relevant documents from a knowledge base, then use those documents \n",
    "        as context for a language model to generate accurate and informed responses. This approach helps reduce hallucinations \n",
    "        and provides more factual, grounded answers. RAG is particularly useful for building AI systems that need to answer \n",
    "        questions based on specific, up-to-date, or proprietary information.\"\"\",\n",
    "        metadata={\"source\": \"rag.txt\", \"topic\": \"RAG\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"\"\"Vector databases are specialized databases designed to store and efficiently search through \n",
    "        high-dimensional vector embeddings. These embeddings are numerical representations of data such as text, images, \n",
    "        or audio. Vector databases use similarity search algorithms like cosine similarity or euclidean distance to find \n",
    "        the most relevant vectors. Popular vector databases include FAISS, Pinecone, Weaviate, and ChromaDB. They are \n",
    "        essential components of modern RAG systems and semantic search applications.\"\"\",\n",
    "        metadata={\"source\": \"vector_db.txt\", \"topic\": \"Vector Databases\"}\n",
    "    )\n",
    "]\n",
    "\n",
    "print(f\"Loaded {len(documents)} documents\")\n",
    "print(\"\\nDocument sources:\")\n",
    "for i, doc in enumerate(documents, 1):\n",
    "    print(f\"{i}. {doc.metadata['source']} - {doc.metadata['topic']}\")\n",
    "    print(f\"   Content preview: {doc.page_content[:100]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Split Documents into Chunks\n",
    "Breaking down documents into smaller chunks for better retrieval and processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 6 documents into 10 chunks\n",
      "\n",
      "First 3 chunks:\n",
      "\n",
      "--- Chunk 1 ---\n",
      "Source: ai_basics.txt\n",
      "Length: 489 characters\n",
      "Content: Artificial Intelligence (AI) is the simulation of human intelligence processes by machines, \n",
      "        especially computer systems. These processes include learning, reasoning, and self-correction. \n",
      "   ...\n",
      "\n",
      "--- Chunk 2 ---\n",
      "Source: machine_learning.txt\n",
      "Length: 446 characters\n",
      "Content: Machine Learning is a subset of artificial intelligence that focuses on the development of \n",
      "        algorithms and statistical models that enable computers to learn and improve from experience without...\n",
      "\n",
      "--- Chunk 3 ---\n",
      "Source: machine_learning.txt\n",
      "Length: 87 characters\n",
      "Content: unlabeled data, and reinforcement learning learns through trial and error with rewards....\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Initialize text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,        # Maximum size of each chunk\n",
    "    chunk_overlap=50,      # Overlap between chunks to maintain context\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    ")\n",
    "\n",
    "# Split documents\n",
    "splits = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"Split {len(documents)} documents into {len(splits)} chunks\")\n",
    "print(\"\\nFirst 3 chunks:\")\n",
    "for i, chunk in enumerate(splits[:3], 1):\n",
    "    print(f\"\\n--- Chunk {i} ---\")\n",
    "    print(f\"Source: {chunk.metadata['source']}\")\n",
    "    print(f\"Length: {len(chunk.page_content)} characters\")\n",
    "    print(f\"Content: {chunk.page_content[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Create Embeddings and FAISS Vector Store\n",
    "Converting text chunks into vector embeddings and storing them in a local FAISS database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using FREE HuggingFace embeddings (all-MiniLM-L6-v2)\n",
      "Creating embeddings and building FAISS vector store...\n",
      "This may take a few moments...\n",
      "\n",
      "✓ FAISS vector store created successfully!\n",
      "✓ Stored 10 document chunks as vector embeddings\n",
      "\n",
      "Vector store is ready for similarity search!\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# ============================================\n",
    "# Choose Embedding Model\n",
    "# ============================================\n",
    "USE_OPENAI_EMBEDDINGS = False  # Set to True if you have OpenAI credits\n",
    "\n",
    "if USE_OPENAI_EMBEDDINGS:\n",
    "    # Option 1: OpenAI Embeddings (requires credits)\n",
    "    embeddings = OpenAIEmbeddings(\n",
    "        model=\"text-embedding-ada-002\"\n",
    "    )\n",
    "    print(\"Using OpenAI embeddings (text-embedding-ada-002)\")\n",
    "else:\n",
    "    # Option 2: Free HuggingFace Embeddings (no API key needed!)\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    )\n",
    "    print(\"Using FREE HuggingFace embeddings (all-MiniLM-L6-v2)\")\n",
    "\n",
    "print(\"Creating embeddings and building FAISS vector store...\")\n",
    "print(\"This may take a few moments...\\n\")\n",
    "\n",
    "# Create FAISS vector store from documents\n",
    "vectorstore = FAISS.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embeddings\n",
    ")\n",
    "\n",
    "print(f\"✓ FAISS vector store created successfully!\")\n",
    "print(f\"✓ Stored {len(splits)} document chunks as vector embeddings\")\n",
    "print(f\"\\nVector store is ready for similarity search!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Retrieve Relevant Chunks\n",
    "Performing similarity search to find the most relevant document chunks for a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is RAG and how does it work?\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Retrieved 3 most relevant chunks:\n",
      "\n",
      "\n",
      "================================================================================\n",
      "RETRIEVED DOCUMENT 1\n",
      "================================================================================\n",
      "Source: vector_db.txt\n",
      "Topic: Vector Databases\n",
      "\n",
      "Content:\n",
      "essential components of modern RAG systems and semantic search applications.\n",
      "\n",
      "================================================================================\n",
      "RETRIEVED DOCUMENT 2\n",
      "================================================================================\n",
      "Source: rag.txt\n",
      "Topic: RAG\n",
      "\n",
      "Content:\n",
      "Retrieval-Augmented Generation (RAG) is a technique that combines information retrieval with \n",
      "        text generation. RAG systems first retrieve relevant documents from a knowledge base, then use those documents \n",
      "        as context for a language model to generate accurate and informed responses. This approach helps reduce hallucinations \n",
      "        and provides more factual, grounded answers. RAG is particularly useful for building AI systems that need to answer\n",
      "\n",
      "================================================================================\n",
      "RETRIEVED DOCUMENT 3\n",
      "================================================================================\n",
      "Source: nlp.txt\n",
      "Topic: NLP\n",
      "\n",
      "Content:\n",
      "Natural Language Processing (NLP) is a branch of artificial intelligence that helps computers \n",
      "        understand, interpret, and manipulate human language. NLP combines computational linguistics with statistical, \n",
      "        machine learning, and deep learning models. Applications of NLP include machine translation, sentiment analysis, \n",
      "        chatbots, text summarization, and question answering systems. Modern NLP has been revolutionized by transformer \n",
      "        models like BERT and GPT.\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Define the query\n",
    "query = \"What is RAG and how does it work?\"\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Retrieve top-k most similar documents\n",
    "k = 3  # Number of documents to retrieve\n",
    "retrieved_docs = vectorstore.similarity_search(query, k=k)\n",
    "\n",
    "print(f\"\\nRetrieved {len(retrieved_docs)} most relevant chunks:\\n\")\n",
    "\n",
    "for i, doc in enumerate(retrieved_docs, 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"RETRIEVED DOCUMENT {i}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Source: {doc.metadata['source']}\")\n",
    "    print(f\"Topic: {doc.metadata['topic']}\")\n",
    "    print(f\"\\nContent:\\n{doc.page_content}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Generate Answer Using LLM\n",
    "Using a chat model to generate a final answer based on the retrieved context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Anthropic Claude-3.5-Haiku (fast & affordable!)\n",
      "\n",
      "✓ RAG chain created successfully!\n",
      "✓ Ready to answer questions!\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Initialize the chat model\n",
    "if USE_OPENAI:\n",
    "    llm = ChatOpenAI(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        temperature=0\n",
    "    )\n",
    "    print(\"Using OpenAI GPT-3.5-turbo\")\n",
    "else:\n",
    "    # Using Claude 3.5 Haiku - fast, affordable, and generous free tier!\n",
    "    llm = ChatAnthropic(\n",
    "        model=\"claude-3-5-haiku-20241022\",\n",
    "        temperature=0\n",
    "    )\n",
    "    print(\"Using Anthropic Claude-3.5-Haiku (fast & affordable!)\")\n",
    "\n",
    "# Create a custom prompt template\n",
    "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. \n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: Let me provide a detailed answer based on the context above.\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# Create retrieval QA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 3}),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": PROMPT}\n",
    ")\n",
    "\n",
    "print(\"\\n✓ RAG chain created successfully!\")\n",
    "print(\"✓ Ready to answer questions!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Display Complete Results\n",
    "Showing the query, retrieved context, and final generated answer together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "RAG PIPELINE - COMPLETE RESULTS\n",
      "================================================================================\n",
      "\n",
      "📝 QUERY:\n",
      "What is RAG and how does it work?\n",
      "\n",
      "================================================================================\n",
      "📚 RETRIEVED CONTEXT (Top 3 Chunks)\n",
      "================================================================================\n",
      "\n",
      "--- Context Chunk 1 ---\n",
      "Source: vector_db.txt\n",
      "Topic: Vector Databases\n",
      "\n",
      "Content:\n",
      "essential components of modern RAG systems and semantic search applications.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Context Chunk 2 ---\n",
      "Source: rag.txt\n",
      "Topic: RAG\n",
      "\n",
      "Content:\n",
      "Retrieval-Augmented Generation (RAG) is a technique that combines information retrieval with \n",
      "        text generation. RAG systems first retrieve relevant documents from a knowledge base, then use those documents \n",
      "        as context for a language model to generate accurate and informed responses. This approach helps reduce hallucinations \n",
      "        and provides more factual, grounded answers. RAG is particularly useful for building AI systems that need to answer\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Context Chunk 3 ---\n",
      "Source: nlp.txt\n",
      "Topic: NLP\n",
      "\n",
      "Content:\n",
      "Natural Language Processing (NLP) is a branch of artificial intelligence that helps computers \n",
      "        understand, interpret, and manipulate human language. NLP combines computational linguistics with statistical, \n",
      "        machine learning, and deep learning models. Applications of NLP include machine translation, sentiment analysis, \n",
      "        chatbots, text summarization, and question answering systems. Modern NLP has been revolutionized by transformer \n",
      "        models like BERT and GPT.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "🤖 GENERATED ANSWER\n",
      "================================================================================\n",
      "\n",
      "Retrieval-Augmented Generation (RAG) is a technique that combines information retrieval with text generation. Here's how it works:\n",
      "\n",
      "1. First, the RAG system retrieves relevant documents from a knowledge base.\n",
      "\n",
      "2. Then, it uses these retrieved documents as context for a language model to generate responses.\n",
      "\n",
      "The key benefits of RAG are:\n",
      "- It helps reduce hallucinations (false or made-up information)\n",
      "- Provides more factual and grounded answers\n",
      "- Allows AI systems to generate more accurate responses by drawing from a specific set of reference documents\n",
      "\n",
      "The process essentially enhances the language model's ability to generate informed and contextually accurate responses by first pulling in relevant information before generating an answer.\n",
      "\n",
      "================================================================================\n",
      "✅ RAG Pipeline Execution Completed Successfully!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Run the query through the RAG pipeline\n",
    "result = qa_chain.invoke({\"query\": query})\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RAG PIPELINE - COMPLETE RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n📝 QUERY:\\n{query}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"📚 RETRIEVED CONTEXT (Top 3 Chunks)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, doc in enumerate(result['source_documents'], 1):\n",
    "    print(f\"\\n--- Context Chunk {i} ---\")\n",
    "    print(f\"Source: {doc.metadata['source']}\")\n",
    "    print(f\"Topic: {doc.metadata['topic']}\")\n",
    "    print(f\"\\nContent:\\n{doc.page_content}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"🤖 GENERATED ANSWER\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n{result['result']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✅ RAG Pipeline Execution Completed Successfully!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Example Queries\n",
    "Try the RAG system with different questions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing RAG with additional queries...\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Example Query 1: What are the three types of machine learning?\n",
      "================================================================================\n",
      "\n",
      "Answer:\n",
      "According to the context, the three main types of machine learning are:\n",
      "\n",
      "1. Supervised learning (which uses labeled data)\n",
      "2. Unsupervised learning (which finds patterns in data)\n",
      "3. Reinforcement learning\n",
      "\n",
      "Note: While the context was cut off mid-sentence for both supervised and unsupervised learning descriptions, these three types are explicitly stated in the first paragraph about Machine Learning.\n",
      "\n",
      "Sources used: machine_learning.txt, ai_basics.txt, deep_learning.txt\n",
      "\n",
      "================================================================================\n",
      "Example Query 2: Explain what vector databases are used for\n",
      "================================================================================\n",
      "\n",
      "Answer:\n",
      "Based on the context provided, vector databases are specialized databases designed to store and efficiently search through high-dimensional vector embeddings. These embeddings are numerical representations of various types of data like text, images, or audio.\n",
      "\n",
      "The primary purpose of vector databases is to enable similarity search using algorithms like cosine similarity or euclidean distance. This means they can help find the most relevant or similar vectors in a large dataset quickly and efficiently.\n",
      "\n",
      "Some practical applications might include:\n",
      "- Finding similar documents or texts\n",
      "- Recommending similar images or content\n",
      "- Enabling semantic search capabilities\n",
      "- Supporting machine learning and AI applications that require quick retrieval of similar data points\n",
      "\n",
      "The context mentions some popular vector databases like FAISS, Pinecone, Weaviate, and ChromaDB, and notes that they require large amounts of data and computational power to train effectively.\n",
      "\n",
      "Sources used: vector_db.txt, machine_learning.txt, deep_learning.txt\n",
      "\n",
      "================================================================================\n",
      "Example Query 3: What is the difference between AI and deep learning?\n",
      "================================================================================\n",
      "\n",
      "Answer:\n",
      "Based on the context provided, here's the difference between AI and deep learning:\n",
      "\n",
      "Artificial Intelligence (AI) is a broader concept that involves simulating human intelligence processes by machines, including learning, reasoning, and self-correction. It encompasses various technologies and approaches to making machines perform tasks that typically require human intelligence.\n",
      "\n",
      "Deep Learning, on the other hand, is a specialized subset of machine learning that uses neural networks with multiple layers. It is a more specific technique within the broader field of AI, particularly focused on using deep neural networks that are inspired by the structure of the human brain. Deep learning is especially effective in specific areas like image recognition, natural language processing, and game playing.\n",
      "\n",
      "To put it simply, deep learning is a specific approach or technique within the broader field of artificial intelligence, much like machine learning is also a subset of AI. Deep learning represents a more advanced and complex method of machine learning that uses multi-layered neural networks to learn and make decisions.\n",
      "\n",
      "Sources used: deep_learning.txt, ai_basics.txt, machine_learning.txt\n"
     ]
    }
   ],
   "source": [
    "# Try additional queries\n",
    "example_queries = [\n",
    "    \"What are the three types of machine learning?\",\n",
    "    \"Explain what vector databases are used for\",\n",
    "    \"What is the difference between AI and deep learning?\"\n",
    "]\n",
    "\n",
    "print(\"Testing RAG with additional queries...\\n\")\n",
    "\n",
    "for i, test_query in enumerate(example_queries, 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Example Query {i}: {test_query}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    test_result = qa_chain.invoke({\"query\": test_query})\n",
    "    \n",
    "    print(f\"\\nAnswer:\\n{test_result['result']}\")\n",
    "    print(f\"\\nSources used: {', '.join([doc.metadata['source'] for doc in test_result['source_documents']])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated a complete RAG pipeline:\n",
    "\n",
    "✅ **Step 1**: Verified required libraries (pre-installed in virtual environment)  \n",
    "✅ **Step 2**: Configured API keys for LLM access  \n",
    "✅ **Step 3**: Loaded 6 sample documents about AI topics  \n",
    "✅ **Step 4**: Split documents into manageable chunks  \n",
    "✅ **Step 5**: Created vector embeddings and stored in FAISS  \n",
    "✅ **Step 6**: Retrieved top-k relevant chunks using similarity search  \n",
    "✅ **Step 7**: Generated answers using OpenAI/Claude LLM  \n",
    "✅ **Step 8**: Displayed complete results with context and answer  \n",
    "\n",
    "**Key Components:**\n",
    "- **Vector Store**: FAISS (local, no cloud required)\n",
    "- **Embeddings**: OpenAI text-embedding-ada-002\n",
    "- **LLM**: OpenAI GPT-3.5-turbo or Anthropic Claude\n",
    "- **Framework**: LangChain\n",
    "\n",
    "**Screenshot Checklist for Assignment:**\n",
    "1. ✓ Libraries verified (Cell 1)\n",
    "2. ✓ Documents loaded and split (Cells 3-4)\n",
    "3. ✓ Embeddings created (Cell 5)\n",
    "4. ✓ FAISS vector store created (Cell 5)\n",
    "5. ✓ Query retrieval results (Cell 6)\n",
    "6. ✓ Final generated answer (Cell 8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
