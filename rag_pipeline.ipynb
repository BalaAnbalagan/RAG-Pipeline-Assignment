{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval-Augmented Generation (RAG) Pipeline\n",
    "## AI Engineering Assignment\n",
    "\n",
    "This notebook demonstrates a complete RAG pipeline using LangChain:\n",
    "1. Load sample documents\n",
    "2. Split into chunks\n",
    "3. Create embeddings\n",
    "4. Store in FAISS vector database\n",
    "5. Retrieve relevant chunks\n",
    "6. Generate answer using LLM\n",
    "\n",
    "**Important Setup:**\n",
    "- Make sure you're using the **\"RAG Assignment (Python 3.9)\"** kernel\n",
    "- All packages are pre-installed in the virtual environment\n",
    "- You'll need an OpenAI API key or Anthropic API key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Verify Libraries\n",
    "\n",
    "All packages are pre-installed in the dedicated virtual environment. Let's verify they're available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify installations (packages are pre-installed in virtual environment)\n",
    "import langchain\n",
    "import faiss\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "print(\"‚úì All libraries loaded successfully!\")\n",
    "print(f\"‚úì LangChain version: {langchain.__version__}\")\n",
    "print(f\"‚úì FAISS available\")\n",
    "print(f\"\\nReady to build RAG pipeline!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Configure API Keys\n",
    "\n",
    "You'll need an API key for the chat model (Claude or OpenAI).\n",
    "\n",
    "**Recommendation**: Use **Anthropic Claude** - more generous free tier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "# ============================================\n",
    "# Choose which LLM API to use\n",
    "# ============================================\n",
    "USE_OPENAI = False  # Set to True to use OpenAI instead of Claude\n",
    "\n",
    "# For embeddings, we use FREE HuggingFace (no API key needed)\n",
    "# For chat LLM, you need either Anthropic or OpenAI API key\n",
    "\n",
    "if USE_OPENAI:\n",
    "    if \"OPENAI_API_KEY\" not in os.environ:\n",
    "        os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")\n",
    "    print(\"‚úì OpenAI API key configured\")\n",
    "    print(\"  Using: GPT-3.5-turbo for chat\")\n",
    "else:\n",
    "    if \"ANTHROPIC_API_KEY\" not in os.environ:\n",
    "        os.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass(\"Enter your Anthropic API key: \")\n",
    "    print(\"‚úì Anthropic API key configured\")\n",
    "    print(\"  Using: Claude-3.5-Haiku for chat (fast & affordable)\")\n",
    "\n",
    "print(\"\\nNote: Embeddings use FREE HuggingFace (no API credits needed!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load Sample Documents\n",
    "Creating sample text documents about AI topics for our knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "# Create sample documents about AI and Machine Learning\n",
    "documents = [\n",
    "    Document(\n",
    "        page_content=\"\"\"Artificial Intelligence (AI) is the simulation of human intelligence processes by machines, \n",
    "        especially computer systems. These processes include learning, reasoning, and self-correction. \n",
    "        AI applications include expert systems, natural language processing, speech recognition, and machine vision. \n",
    "        AI has become increasingly important in modern technology and is used in various industries including \n",
    "        healthcare, finance, transportation, and entertainment.\"\"\",\n",
    "        metadata={\"source\": \"ai_basics.txt\", \"topic\": \"AI Introduction\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"\"\"Machine Learning is a subset of artificial intelligence that focuses on the development of \n",
    "        algorithms and statistical models that enable computers to learn and improve from experience without being \n",
    "        explicitly programmed. There are three main types of machine learning: supervised learning, unsupervised learning, \n",
    "        and reinforcement learning. Supervised learning uses labeled data, unsupervised learning finds patterns in \n",
    "        unlabeled data, and reinforcement learning learns through trial and error with rewards.\"\"\",\n",
    "        metadata={\"source\": \"machine_learning.txt\", \"topic\": \"Machine Learning\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"\"\"Deep Learning is a specialized subset of machine learning that uses neural networks with \n",
    "        multiple layers (deep neural networks). These networks are inspired by the structure and function of the human brain. \n",
    "        Deep learning has achieved remarkable success in areas such as image recognition, natural language processing, \n",
    "        and game playing. Popular deep learning frameworks include TensorFlow, PyTorch, and Keras. Deep learning models \n",
    "        require large amounts of data and computational power to train effectively.\"\"\",\n",
    "        metadata={\"source\": \"deep_learning.txt\", \"topic\": \"Deep Learning\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"\"\"Natural Language Processing (NLP) is a branch of artificial intelligence that helps computers \n",
    "        understand, interpret, and manipulate human language. NLP combines computational linguistics with statistical, \n",
    "        machine learning, and deep learning models. Applications of NLP include machine translation, sentiment analysis, \n",
    "        chatbots, text summarization, and question answering systems. Modern NLP has been revolutionized by transformer \n",
    "        models like BERT and GPT.\"\"\",\n",
    "        metadata={\"source\": \"nlp.txt\", \"topic\": \"NLP\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"\"\"Retrieval-Augmented Generation (RAG) is a technique that combines information retrieval with \n",
    "        text generation. RAG systems first retrieve relevant documents from a knowledge base, then use those documents \n",
    "        as context for a language model to generate accurate and informed responses. This approach helps reduce hallucinations \n",
    "        and provides more factual, grounded answers. RAG is particularly useful for building AI systems that need to answer \n",
    "        questions based on specific, up-to-date, or proprietary information.\"\"\",\n",
    "        metadata={\"source\": \"rag.txt\", \"topic\": \"RAG\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"\"\"Vector databases are specialized databases designed to store and efficiently search through \n",
    "        high-dimensional vector embeddings. These embeddings are numerical representations of data such as text, images, \n",
    "        or audio. Vector databases use similarity search algorithms like cosine similarity or euclidean distance to find \n",
    "        the most relevant vectors. Popular vector databases include FAISS, Pinecone, Weaviate, and ChromaDB. They are \n",
    "        essential components of modern RAG systems and semantic search applications.\"\"\",\n",
    "        metadata={\"source\": \"vector_db.txt\", \"topic\": \"Vector Databases\"}\n",
    "    )\n",
    "]\n",
    "\n",
    "print(f\"Loaded {len(documents)} documents\")\n",
    "print(\"\\nDocument sources:\")\n",
    "for i, doc in enumerate(documents, 1):\n",
    "    print(f\"{i}. {doc.metadata['source']} - {doc.metadata['topic']}\")\n",
    "    print(f\"   Content preview: {doc.page_content[:100]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Split Documents into Chunks\n",
    "Breaking down documents into smaller chunks for better retrieval and processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Initialize text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,        # Maximum size of each chunk\n",
    "    chunk_overlap=50,      # Overlap between chunks to maintain context\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    ")\n",
    "\n",
    "# Split documents\n",
    "splits = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"Split {len(documents)} documents into {len(splits)} chunks\")\n",
    "print(\"\\nFirst 3 chunks:\")\n",
    "for i, chunk in enumerate(splits[:3], 1):\n",
    "    print(f\"\\n--- Chunk {i} ---\")\n",
    "    print(f\"Source: {chunk.metadata['source']}\")\n",
    "    print(f\"Length: {len(chunk.page_content)} characters\")\n",
    "    print(f\"Content: {chunk.page_content[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Create Embeddings and FAISS Vector Store\n",
    "Converting text chunks into vector embeddings and storing them in a local FAISS database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# ============================================\n",
    "# Choose Embedding Model\n",
    "# ============================================\n",
    "USE_OPENAI_EMBEDDINGS = False  # Set to True if you have OpenAI credits\n",
    "\n",
    "if USE_OPENAI_EMBEDDINGS:\n",
    "    # Option 1: OpenAI Embeddings (requires credits)\n",
    "    embeddings = OpenAIEmbeddings(\n",
    "        model=\"text-embedding-ada-002\"\n",
    "    )\n",
    "    print(\"Using OpenAI embeddings (text-embedding-ada-002)\")\n",
    "else:\n",
    "    # Option 2: Free HuggingFace Embeddings (no API key needed!)\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    )\n",
    "    print(\"Using FREE HuggingFace embeddings (all-MiniLM-L6-v2)\")\n",
    "\n",
    "print(\"Creating embeddings and building FAISS vector store...\")\n",
    "print(\"This may take a few moments...\\n\")\n",
    "\n",
    "# Create FAISS vector store from documents\n",
    "vectorstore = FAISS.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embeddings\n",
    ")\n",
    "\n",
    "print(f\"‚úì FAISS vector store created successfully!\")\n",
    "print(f\"‚úì Stored {len(splits)} document chunks as vector embeddings\")\n",
    "print(f\"\\nVector store is ready for similarity search!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Retrieve Relevant Chunks\n",
    "Performing similarity search to find the most relevant document chunks for a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the query\n",
    "query = \"What is RAG and how does it work?\"\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Retrieve top-k most similar documents\n",
    "k = 3  # Number of documents to retrieve\n",
    "retrieved_docs = vectorstore.similarity_search(query, k=k)\n",
    "\n",
    "print(f\"\\nRetrieved {len(retrieved_docs)} most relevant chunks:\\n\")\n",
    "\n",
    "for i, doc in enumerate(retrieved_docs, 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"RETRIEVED DOCUMENT {i}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Source: {doc.metadata['source']}\")\n",
    "    print(f\"Topic: {doc.metadata['topic']}\")\n",
    "    print(f\"\\nContent:\\n{doc.page_content}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Generate Answer Using LLM\n",
    "Using a chat model to generate a final answer based on the retrieved context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Initialize the chat model\n",
    "if USE_OPENAI:\n",
    "    llm = ChatOpenAI(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        temperature=0\n",
    "    )\n",
    "    print(\"Using OpenAI GPT-3.5-turbo\")\n",
    "else:\n",
    "    # Using Claude 3.5 Haiku - fast, affordable, and generous free tier!\n",
    "    llm = ChatAnthropic(\n",
    "        model=\"claude-3-5-haiku-20241022\",\n",
    "        temperature=0\n",
    "    )\n",
    "    print(\"Using Anthropic Claude-3.5-Haiku (fast & affordable!)\")\n",
    "\n",
    "# Create a custom prompt template\n",
    "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. \n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: Let me provide a detailed answer based on the context above.\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# Create retrieval QA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 3}),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": PROMPT}\n",
    ")\n",
    "\n",
    "print(\"\\n‚úì RAG chain created successfully!\")\n",
    "print(\"‚úì Ready to answer questions!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Display Complete Results\n",
    "Showing the query, retrieved context, and final generated answer together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the query through the RAG pipeline\n",
    "result = qa_chain.invoke({\"query\": query})\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RAG PIPELINE - COMPLETE RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüìù QUERY:\\n{query}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìö RETRIEVED CONTEXT (Top 3 Chunks)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, doc in enumerate(result['source_documents'], 1):\n",
    "    print(f\"\\n--- Context Chunk {i} ---\")\n",
    "    print(f\"Source: {doc.metadata['source']}\")\n",
    "    print(f\"Topic: {doc.metadata['topic']}\")\n",
    "    print(f\"\\nContent:\\n{doc.page_content}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ü§ñ GENERATED ANSWER\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n{result['result']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ RAG Pipeline Execution Completed Successfully!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Example Queries\n",
    "Try the RAG system with different questions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try additional queries\n",
    "example_queries = [\n",
    "    \"What are the three types of machine learning?\",\n",
    "    \"Explain what vector databases are used for\",\n",
    "    \"What is the difference between AI and deep learning?\"\n",
    "]\n",
    "\n",
    "print(\"Testing RAG with additional queries...\\n\")\n",
    "\n",
    "for i, test_query in enumerate(example_queries, 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Example Query {i}: {test_query}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    test_result = qa_chain.invoke({\"query\": test_query})\n",
    "    \n",
    "    print(f\"\\nAnswer:\\n{test_result['result']}\")\n",
    "    print(f\"\\nSources used: {', '.join([doc.metadata['source'] for doc in test_result['source_documents']])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated a complete RAG pipeline:\n",
    "\n",
    "‚úÖ **Step 1**: Verified required libraries (pre-installed in virtual environment)  \n",
    "‚úÖ **Step 2**: Configured API keys for LLM access  \n",
    "‚úÖ **Step 3**: Loaded 6 sample documents about AI topics  \n",
    "‚úÖ **Step 4**: Split documents into manageable chunks  \n",
    "‚úÖ **Step 5**: Created vector embeddings and stored in FAISS  \n",
    "‚úÖ **Step 6**: Retrieved top-k relevant chunks using similarity search  \n",
    "‚úÖ **Step 7**: Generated answers using OpenAI/Claude LLM  \n",
    "‚úÖ **Step 8**: Displayed complete results with context and answer  \n",
    "\n",
    "**Key Components:**\n",
    "- **Vector Store**: FAISS (local, no cloud required)\n",
    "- **Embeddings**: OpenAI text-embedding-ada-002\n",
    "- **LLM**: OpenAI GPT-3.5-turbo or Anthropic Claude\n",
    "- **Framework**: LangChain\n",
    "\n",
    "**Screenshot Checklist for Assignment:**\n",
    "1. ‚úì Libraries verified (Cell 1)\n",
    "2. ‚úì Documents loaded and split (Cells 3-4)\n",
    "3. ‚úì Embeddings created (Cell 5)\n",
    "4. ‚úì FAISS vector store created (Cell 5)\n",
    "5. ‚úì Query retrieval results (Cell 6)\n",
    "6. ‚úì Final generated answer (Cell 8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
